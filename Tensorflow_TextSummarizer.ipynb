{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tensorflow_TextSummarizer.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnTpBvSY2vql",
        "colab_type": "code",
        "outputId": "2b245abb-7d18-4a10-e81e-5f0e4bb06735",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        }
      },
      "source": [
        "!ls -la"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 24\n",
            "drwxr-xr-x 1 root root 4096 May  6 02:45 .\n",
            "drwxr-xr-x 1 root root 4096 May  6 02:00 ..\n",
            "drwxr-xr-x 1 root root 4096 Apr 29 16:32 .config\n",
            "drwx------ 3 root root 4096 May  6 02:44 drive\n",
            "drwxr-xr-x 3 root root 4096 May  6 02:39 embeddings\n",
            "drwxr-xr-x 1 root root 4096 Apr 29 16:32 sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4Qn-JxreCsu",
        "colab_type": "code",
        "outputId": "2443d74b-19e0-4684-fab1-895418bbe14a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 579
        }
      },
      "source": [
        "!pip install tensorflow-gpu==2.0.0-alpha0"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==2.0.0-alpha0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/66/32cffad095253219d53f6b6c2a436637bbe45ac4e7be0244557210dc3918/tensorflow_gpu-2.0.0a0-cp36-cp36m-manylinux1_x86_64.whl (332.1MB)\n",
            "\u001b[K     |████████████████████████████████| 332.1MB 66kB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.7.1)\n",
            "Collecting google-pasta>=0.1.2 (from tensorflow-gpu==2.0.0-alpha0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/bb/f1bbc131d6294baa6085a222d29abadd012696b73dcbf8cf1bf56b9f082a/google_pasta-0.1.5-py3-none-any.whl (51kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 29.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.16.3)\n",
            "Collecting tb-nightly<1.14.0a20190302,>=1.14.0a20190301 (from tensorflow-gpu==2.0.0-alpha0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/51/aa1d756644bf4624c03844115e4ac4058eff77acd786b26315f051a4b195/tb_nightly-1.14.0a20190301-py3-none-any.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 36.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (3.7.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.33.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.2.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.7.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.12.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.0.7)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.0.9)\n",
            "Collecting tf-estimator-nightly<1.14.0.dev2019030116,>=1.14.0.dev2019030115 (from tensorflow-gpu==2.0.0-alpha0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/82/f16063b4eed210dc2ab057930ac1da4fbe1e91b7b051a6c8370b401e6ae7/tf_estimator_nightly-1.14.0.dev2019030115-py2.py3-none-any.whl (411kB)\n",
            "\u001b[K     |████████████████████████████████| 419kB 49.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0-alpha0) (3.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0-alpha0) (0.15.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0-alpha0) (41.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==2.0.0-alpha0) (2.8.0)\n",
            "Installing collected packages: google-pasta, tb-nightly, tf-estimator-nightly, tensorflow-gpu\n",
            "Successfully installed google-pasta-0.1.5 tb-nightly-1.14.0a20190301 tensorflow-gpu-2.0.0a0 tf-estimator-nightly-1.14.0.dev2019030115\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-i55kIMN2x0k",
        "colab_type": "text"
      },
      "source": [
        "**Step 1:  GloVe as Tensorflow embedding layer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2DqrEax3khm",
        "colab_type": "code",
        "outputId": "ef9ec723-fdcf-4fdb-baf3-bd3d2e7b3cab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "!pip install chakin"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: chakin in /usr/local/lib/python3.6/dist-packages (0.0.8)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from chakin) (1.12.0)\n",
            "Requirement already satisfied: pandas>=0.20.1 in /usr/local/lib/python3.6/dist-packages (from chakin) (0.24.2)\n",
            "Requirement already satisfied: progressbar2>=3.20.0 in /usr/local/lib/python3.6/dist-packages (from chakin) (3.38.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.20.1->chakin) (2.5.3)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.20.1->chakin) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.20.1->chakin) (1.16.3)\n",
            "Requirement already satisfied: python-utils>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from progressbar2>=3.20.0->chakin) (2.3.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_qoTbuU2750",
        "colab_type": "code",
        "outputId": "a015ab17-3d0d-4ae7-869c-afdb9ad92837",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        }
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import chakin\n",
        "\n",
        "import json\n",
        "import os\n",
        "from collections import defaultdict\n",
        "\n",
        "chakin.search(lang='English')\n",
        "#14        GloVe.6B.300d        300  Wikipedia+Gigaword 5 (6B)           400K\n",
        "# Downloading GloVe.6B.300d embeddings from Stanford:\n",
        "CHAKIN_INDEX = 14\n",
        "NUMBER_OF_DIMENSIONS = 300\n",
        "SUBFOLDER_NAME = \"glove.6B\"\n",
        "\n",
        "DATA_FOLDER = \"embeddings\"\n",
        "ZIP_FILE = os.path.join(DATA_FOLDER, \"{}.zip\".format(SUBFOLDER_NAME))\n",
        "ZIP_FILE_ALT = \"glove\" + ZIP_FILE[5:]  # sometimes it's lowercase only...\n",
        "UNZIP_FOLDER = os.path.join(DATA_FOLDER, SUBFOLDER_NAME)\n",
        "if SUBFOLDER_NAME[-1] == \"d\":\n",
        "    GLOVE_FILENAME = os.path.join(UNZIP_FOLDER, \"{}.txt\".format(SUBFOLDER_NAME))\n",
        "else:\n",
        "    GLOVE_FILENAME = os.path.join(UNZIP_FOLDER, \"{}.{}d.txt\".format(SUBFOLDER_NAME, NUMBER_OF_DIMENSIONS))\n",
        "\n",
        "\n",
        "if not os.path.exists(ZIP_FILE) and not os.path.exists(UNZIP_FOLDER):\n",
        "    # GloVe by Stanford is licensed Apache 2.0: \n",
        "    #     https://github.com/stanfordnlp/GloVe/blob/master/LICENSE\n",
        "    #     http://nlp.stanford.edu/data/glove.6B.zip\n",
        "    #     Copyright 2014 The Board of Trustees of The Leland Stanford Junior University\n",
        "    print(\"Downloading embeddings to '{}'\".format(ZIP_FILE))\n",
        "    chakin.download(number=CHAKIN_INDEX, save_dir='./{}'.format(DATA_FOLDER))\n",
        "else:\n",
        "    print(\"Embeddings already downloaded.\")\n",
        "    \n",
        "if not os.path.exists(UNZIP_FOLDER):\n",
        "    import zipfile\n",
        "    if not os.path.exists(ZIP_FILE) and os.path.exists(ZIP_FILE_ALT):\n",
        "        ZIP_FILE = ZIP_FILE_ALT\n",
        "    with zipfile.ZipFile(ZIP_FILE,\"r\") as zip_ref:\n",
        "        print(\"Extracting embeddings to '{}'\".format(UNZIP_FOLDER))\n",
        "        zip_ref.extractall(UNZIP_FOLDER)\n",
        "else:\n",
        "    print(\"Embeddings already extracted.\")\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                   Name  Dimension                     Corpus VocabularySize  \\\n",
            "2          fastText(en)        300                  Wikipedia           2.5M   \n",
            "11         GloVe.6B.50d         50  Wikipedia+Gigaword 5 (6B)           400K   \n",
            "12        GloVe.6B.100d        100  Wikipedia+Gigaword 5 (6B)           400K   \n",
            "13        GloVe.6B.200d        200  Wikipedia+Gigaword 5 (6B)           400K   \n",
            "14        GloVe.6B.300d        300  Wikipedia+Gigaword 5 (6B)           400K   \n",
            "15       GloVe.42B.300d        300          Common Crawl(42B)           1.9M   \n",
            "16      GloVe.840B.300d        300         Common Crawl(840B)           2.2M   \n",
            "17    GloVe.Twitter.25d         25               Twitter(27B)           1.2M   \n",
            "18    GloVe.Twitter.50d         50               Twitter(27B)           1.2M   \n",
            "19   GloVe.Twitter.100d        100               Twitter(27B)           1.2M   \n",
            "20   GloVe.Twitter.200d        200               Twitter(27B)           1.2M   \n",
            "21  word2vec.GoogleNews        300          Google News(100B)           3.0M   \n",
            "\n",
            "      Method Language    Author  \n",
            "2   fastText  English  Facebook  \n",
            "11     GloVe  English  Stanford  \n",
            "12     GloVe  English  Stanford  \n",
            "13     GloVe  English  Stanford  \n",
            "14     GloVe  English  Stanford  \n",
            "15     GloVe  English  Stanford  \n",
            "16     GloVe  English  Stanford  \n",
            "17     GloVe  English  Stanford  \n",
            "18     GloVe  English  Stanford  \n",
            "19     GloVe  English  Stanford  \n",
            "20     GloVe  English  Stanford  \n",
            "21  word2vec  English    Google  \n",
            "Downloading embeddings to 'embeddings/glove.6B.zip'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Test: 100% ||                                      | Time:  0:00:28  29.3 MiB/s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting embeddings to 'embeddings/glove.6B'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2snIwKKG3f5X",
        "colab_type": "code",
        "outputId": "5fe773c8-7706-4ffc-ef4c-4e351b804b88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        }
      },
      "source": [
        "print(tf.__version__)\n",
        "!ls -la embeddings/glove.6B"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0-alpha0\n",
            "total 2197156\n",
            "drwxr-xr-x 2 root root       4096 May  6 02:39 .\n",
            "drwxr-xr-x 3 root root       4096 May  6 02:39 ..\n",
            "-rw-r--r-- 1 root root  347116733 May  6 02:39 glove.6B.100d.txt\n",
            "-rw-r--r-- 1 root root  693432828 May  6 02:39 glove.6B.200d.txt\n",
            "-rw-r--r-- 1 root root 1037962819 May  6 02:39 glove.6B.300d.txt\n",
            "-rw-r--r-- 1 root root  171350079 May  6 02:39 glove.6B.50d.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlIhjYea6Nmi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_embedding_from_disks(glove_filename, with_indexes=True):\n",
        "    \"\"\"\n",
        "    Read a GloVe txt file. If `with_indexes=True`, we return a tuple of two dictionnaries\n",
        "    `(word_to_index_dict, index_to_embedding_array)`, otherwise we return only a direct \n",
        "    `word_to_embedding_dict` dictionnary mapping from a string to a numpy array.\n",
        "    \"\"\"\n",
        "    if with_indexes:\n",
        "        word_to_index_dict = dict()\n",
        "        index_to_embedding_array = []\n",
        "    else:\n",
        "        word_to_embedding_dict = dict()\n",
        "\n",
        "    \n",
        "    with open(glove_filename, 'r') as glove_file:\n",
        "        for (i, line) in enumerate(glove_file):\n",
        "            \n",
        "            split = line.split(' ')\n",
        "            \n",
        "            word = split[0]\n",
        "            \n",
        "            representation = split[1:]\n",
        "            representation = np.array(\n",
        "                [float(val) for val in representation]\n",
        "            )\n",
        "            \n",
        "            if with_indexes:\n",
        "                word_to_index_dict[word] = i\n",
        "                index_to_embedding_array.append(representation)\n",
        "            else:\n",
        "                word_to_embedding_dict[word] = representation\n",
        "\n",
        "    _WORD_NOT_FOUND = [0.0]* len(representation)  # Empty representation for unknown words.\n",
        "    if with_indexes:\n",
        "        _LAST_INDEX = i + 1\n",
        "        word_to_index_dict = defaultdict(lambda: _LAST_INDEX, word_to_index_dict)\n",
        "        index_to_embedding_array = np.array(index_to_embedding_array + [_WORD_NOT_FOUND])\n",
        "        return word_to_index_dict, index_to_embedding_array\n",
        "    else:\n",
        "        word_to_embedding_dict = defaultdict(lambda: _WORD_NOT_FOUND)\n",
        "        return word_to_embedding_dict\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eh47GoXg6RYW",
        "colab_type": "code",
        "outputId": "76050c97-7b88-474f-b48b-cae76c5db4a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "source": [
        "print(\"Loading embedding from disks...\")\n",
        "word_to_index, index_to_embedding = load_embedding_from_disks(GLOVE_FILENAME, with_indexes=True)\n",
        "print(\"Embedding loaded from disks.\")\n",
        "\n",
        "vocab_size, embedding_dim = index_to_embedding.shape\n",
        "print(\"Embedding is of shape: {}\".format(index_to_embedding.shape))\n",
        "print(\"This means (number of words, number of dimensions per word)\\n\")\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading embedding from disks...\n",
            "Embedding loaded from disks.\n",
            "Embedding is of shape: (400001, 300)\n",
            "This means (number of words, number of dimensions per word)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THmMNJRn66px",
        "colab_type": "text"
      },
      "source": [
        "**Load the pre-trained GloVe embedding in TensorFlow**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dH7GJ7W6_5x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 4000 # Total Vocabulary’s size is 400K words\n",
        "\n",
        "# Define the variable that will hold the embedding:\n",
        "tf_embedding = tf.Variable(\n",
        "    tf.constant(0.0, shape=index_to_embedding.shape),\n",
        "    trainable=False,\n",
        "    name=\"tf_embedding\"\n",
        ")\n",
        "\n",
        "tf_word_ids = tf.Variable(tf.constant(0,shape=[batch_size]),name=\"tf_word_ids\")\n",
        "tf_word_representation_layer = tf.nn.embedding_lookup(\n",
        "    params=tf_embedding,\n",
        "    ids=tf_word_ids\n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "678FZH3O96ot",
        "colab_type": "text"
      },
      "source": [
        "**Sending the embedding to TensorFlow**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKzuIqO699HU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf_embedding = index_to_embedding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jWpswhDIHqF9",
        "colab": {}
      },
      "source": [
        "class Net(tf.keras.Model):\n",
        "  \"\"\"A simple linear model.\"\"\"\n",
        "\n",
        "  def __init__(self,embeddinglist):\n",
        "    super(Net, self).__init__()\n",
        "    self.l1 = tf.keras.layers.Dense(5)\n",
        "    self.bias = embeddinglist\n",
        "\n",
        "  def call(self, x):\n",
        "    return self.l1(x)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7fAKCtx-ERt",
        "colab_type": "text"
      },
      "source": [
        "**Save for later use**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBGSJxRF-Hug",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prefix = SUBFOLDER_NAME + \".\" + str(NUMBER_OF_DIMENSIONS) + \"d\"\n",
        "TF_EMBEDDINGS_FILE_NAME = os.path.join(DATA_FOLDER, prefix + \".ckpt\")\n",
        "DICT_WORD_TO_INDEX_FILE_NAME = os.path.join(DATA_FOLDER, prefix + \".json\")\n",
        "\n",
        "variables_to_save = [tf_embedding]\n",
        "net = Net(tf_embedding)\n",
        "ckpt = tf.train.Checkpoint(net=net)\n",
        "manager = tf.train.CheckpointManager(ckpt, DATA_FOLDER,max_to_keep=5)\n",
        "save_path = manager.save()\n",
        "#print(\"Testing value 3366 from tf_embedding {}\".format(tf_embedding[3366]))\n",
        "print(\"TF embeddings saved to '{}'.\".format(save_path))\n",
        "\n",
        "with open(DICT_WORD_TO_INDEX_FILE_NAME, 'w') as f:\n",
        "    json.dump(word_to_index, f)\n",
        "print(\"word_to_index dict saved to '{}'.\".format(DICT_WORD_TO_INDEX_FILE_NAME))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPLkMUw_-Qnk",
        "colab_type": "code",
        "outputId": "e16602d0-6f54-4454-ebcb-8a3c6283f9d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "source": [
        "!ls -la embeddings"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 841992\n",
            "drwxr-xr-x 3 root root      4096 May  3 14:53 .\n",
            "drwxr-xr-x 1 root root      4096 May  3 14:52 ..\n",
            "drwxr-xr-x 2 root root      4096 May  3 14:54 glove.6B\n",
            "-rw-r--r-- 1 root root 862182613 May  3 14:53 glove.6B.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuZf_8jt-trJ",
        "colab_type": "text"
      },
      "source": [
        "**Reset the Jupyter notebook and load the embedding from disk**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PzShXNK-sBv",
        "colab_type": "code",
        "outputId": "1a1c90e9-908b-4861-8f0b-6580c9161233",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "# Delete variables and restart the Python kernel\n",
        "#%reset\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGcEdhmZ_Hol",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import json\n",
        "from string import punctuation\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import layers\n",
        "import time\n",
        "\n",
        "batch_size = 100 # Any size is accepted\n",
        "word_representations_dimensions = 300  \n",
        "\n",
        "DATA_FOLDER = \"embeddings\"\n",
        "SUBFOLDER_NAME = \"glove.6B\"\n",
        "TF_EMBEDDING_FILE_NAME = \"{}.ckpt\".format(SUBFOLDER_NAME)\n",
        "SUFFIX = SUBFOLDER_NAME + \".\" + str(word_representations_dimensions)\n",
        "TF_EMBEDDINGS_FILE_PATH = os.path.join(DATA_FOLDER, SUFFIX + \"d.ckpt\")\n",
        "DICT_WORD_TO_INDEX_FILE_NAME = os.path.join(DATA_FOLDER, SUFFIX + \"d.json\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4P_gbQxw_Qxn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_word_to_index(dict_word_to_index_file_name):\n",
        "    \"\"\"\n",
        "    Load a `word_to_index` dict mapping words to their id, with a default value\n",
        "    of pointing to the last index when not found, which is the unknown word.\n",
        "    \"\"\"\n",
        "    with open(dict_word_to_index_file_name, 'r') as f:\n",
        "        word_to_index = json.load(f)\n",
        "    _LAST_INDEX = len(word_to_index) - 2  # Why - 2? Open issue?\n",
        "    print(\"word_to_index dict restored from '{}'.\".format(dict_word_to_index_file_name))\n",
        "    word_to_index = defaultdict(lambda: _LAST_INDEX, word_to_index)\n",
        "\n",
        "    return word_to_index\n",
        "\n",
        "def load_embedding_tf(word_to_index, tf_embeddings_file_path, nb_dims):\n",
        "    \"\"\"\n",
        "    Define the embedding tf.Variable and load it.\n",
        "    \"\"\"\n",
        "   \n",
        "    tf_embedding = tf.Variable(tf.zeros(shape=(len(word_to_index)+1, nb_dims)))\n",
        "    #print(\"Shape of the variable to restore = {}\".format(tf_embedding.shape))  \n",
        "    ckpt = tf.train.Checkpoint(var=tf_embedding)   \n",
        "    manager = tf.train.CheckpointManager(ckpt, DATA_FOLDER, max_to_keep=3)\n",
        "    ckpt.restore(manager.latest_checkpoint)   \n",
        "    print(\"TF 2.0 embeddings restored from '{}'.\".format(manager.latest_checkpoint))\n",
        "    #print(\"embedding restored has shape={}\".format(tf_embedding.shape))\n",
        "    #print(\"load_embedding_tf: Test restored value 3366 from tf_embedding {}\".format(tf_embedding[3366]))\n",
        "    return tf_embedding\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcvkg09x_WEM",
        "colab_type": "text"
      },
      "source": [
        "**Restore the embeddings from disk**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ka4Y_9KCEcfN",
        "colab_type": "code",
        "outputId": "89a0100e-e7a8-41a4-896e-ca5edcd502c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "!ls -la embeddings"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 1318264\n",
            "drwxr-xr-x 3 root root      4096 May  3 07:12 .\n",
            "drwxr-xr-x 1 root root      4096 May  3 06:41 ..\n",
            "-rw-r--r-- 1 root root       166 May  3 07:12 checkpoint\n",
            "-rw-r--r-- 1 root root 480001398 May  3 07:12 ckpt-1.data-00000-of-00001\n",
            "-rw-r--r-- 1 root root       271 May  3 07:12 ckpt-1.index\n",
            "drwxr-xr-x 2 root root      4096 May  3 06:43 glove.6B\n",
            "-rw-r--r-- 1 root root   7691463 May  3 07:12 glove.6B.300d.json\n",
            "-rw-r--r-- 1 root root 862182613 May  3 06:43 glove.6B.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zFioWfkCXPK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the embedding matrix in tf\n",
        "word_to_index = load_word_to_index(\n",
        "    DICT_WORD_TO_INDEX_FILE_NAME)\n",
        "tf_embedding = load_embedding_tf(\n",
        "    word_to_index,\n",
        "    TF_EMBEDDINGS_FILE_PATH, \n",
        "    word_representations_dimensions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIEGxGCIpBoA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#longtext can be full text of the paper (max = 5000 words) or the abstract (250 words)\n",
        "def text_to_word_ids(longtext, word_to_index):\n",
        "    \"\"\"\n",
        "    Note: there might be a better way to split sentences for GloVe.\n",
        "    Please look at the documentation or open an issue to suggest a fix.\n",
        "    \"\"\"\n",
        "    # Separating punctuation from words:\n",
        "    for punctuation_character in punctuation:\n",
        "        longtext = longtext.replace(punctuation_character, \" {} \".format(punctuation_character))\n",
        "    # Removing double spaces and lowercasing:\n",
        "    longtext = longtext.replace(\"  \", \" \").replace(\"  \", \" \").lower().strip()\n",
        "    # Splitting on every space:\n",
        "    split_longtext = longtext.split(\" \")\n",
        "    # Converting to IDs:\n",
        "    ids = [word_to_index[w.strip()] for w in split_longtext]\n",
        "    return ids, split_longtext\n",
        "def ids_to_embeddings(wordids):\n",
        "    embeddings_tensor = [tf_embedding[wordid] for wordid in wordids]\n",
        "    return embeddings_tensor\n",
        "def ids_to_embeddings_V2(wordids,padToSize):\n",
        "    # no padding operation    \n",
        "    if padToSize <= 0:\n",
        "      embeddings_tensor = [tf_embedding[wordid] for wordid in wordids]\n",
        "    else:\n",
        "      wordlistLength = len (wordids)      \n",
        "      remainNumberToPad = padToSize - wordlistLength\n",
        "      if remainNumberToPad < 0:\n",
        "        # wordList is already longer than padToSize => Trim to padToSize\n",
        "        for wordid in wordids:\n",
        "          embeddings_tensor.extend(tf_embedding[wordid])\n",
        "      elif remainNumberToPad == 0:\n",
        "        #wordList equals padToSize then do as no padding\n",
        "        embeddings_tensor = [tf_embedding[wordid] for wordid in wordids]\n",
        "      else:\n",
        "        #wordList is less than padToSize then do the padding        \n",
        "        tempList = []\n",
        "        for i in range(0,wordlistLength):\n",
        "            tempList.append(wordids[i])\n",
        "        for j in range(wordlistLength,padToSize):\n",
        "            #pad the UNKNOW_WORD index\n",
        "            tempList.append(400000) \n",
        "        embeddings_tensor = [tf_embedding[wordid] for wordid in tempList]       \n",
        "    return embeddings_tensor  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAFF2CNvria0",
        "colab_type": "text"
      },
      "source": [
        "**Test function Text2WordIDs:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAfEjVEyrla7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "strFullPaper = \"The world is beautiful and today is a nice day!\"\n",
        "strAbstract = \"Beautiful and nice day.\"\n",
        "\n",
        "word_ids_list, resulttext = text_to_word_ids(strAbstract,word_to_index)\n",
        "\n",
        "print(\"Text to vector with content = \\\"{}\\\":\".format(strAbstract))\n",
        "\n",
        "for word, wordid in zip(resulttext, word_ids_list):\n",
        "\tprint(\"     {}{}\".format((word+ \":\").ljust(15),str(wordid)))\n",
        "  \n",
        "test_abstract_embedding = ids_to_embeddings(word_ids_list)\n",
        "print(\"test_abstract_embedding len={}\".format(len(test_abstract_embedding)))\n",
        "print(\"test_abstract_embedding has shape = {}\".format(len(test_abstract_embedding)))\n",
        "print(\"test_abstract_embedding, each word is a tensor = {}\".format(test_abstract_embedding[0]))\n",
        "print(\"test_abstract_embedding, each word has shape = {}\".format(test_abstract_embedding[0].shape))\n",
        "print(\"test_abstract_embedding, each word has dtype = {}\".format(test_abstract_embedding[0].dtype))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bkj1R8n1CcEN",
        "colab_type": "text"
      },
      "source": [
        "**Load sample pair of 2 text files (FullPaper.txt; Abstract.txt) using DataSet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NduiWYIDCdD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3bnAgFOfZUq",
        "colab_type": "code",
        "outputId": "3de00c12-376a-49cc-c0d0-26f247fbc667",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "!ls -la \"drive/My Drive/Tensorflow/Data/\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 12\n",
            "drwx------ 2 root root 4096 May  2 03:31 DataSet\n",
            "drwx------ 2 root root 4096 Apr 26 09:58 Testing\n",
            "drwx------ 2 root root 4096 Apr 26 09:59 Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-0vXVBFmVAE",
        "colab_type": "code",
        "outputId": "ae1e81fa-877f-4114-a686-2615cedeeb2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "DATASET_FOLDER = \"drive/My Drive/Tensorflow/Data/DataSet\"\n",
        "TRAIN_DATA_FOLDER = \"drive/My Drive/Tensorflow/Data/Training\"\n",
        "TRAIN_DATA_FILE = \"Training.zip\"\n",
        "TRAIN_DATA_FILE_PATH = os.path.join(DATASET_FOLDER, TRAIN_DATA_FILE)\n",
        "import zipfile\n",
        "\n",
        "UNZIP_DATA_FOLDER = TRAIN_DATA_FOLDER\n",
        "with zipfile.ZipFile(TRAIN_DATA_FILE_PATH,\"r\") as zip_ref:\n",
        "        print(\"Extracting training data to '{}'\".format(UNZIP_DATA_FOLDER))\n",
        "        zip_ref.extractall(UNZIP_DATA_FOLDER)\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting training data to 'drive/My Drive/Tensorflow/Data/Training'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13IQjJevmoun",
        "colab_type": "code",
        "outputId": "e6a84299-922d-417c-dbf5-e1d38d3ee65f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "source": [
        "!ls -la \"drive/My Drive/Tensorflow/Data/Training\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 28\n",
            "-rw------- 1 root root   898 May  4 03:56 A-1.txt\n",
            "-rw------- 1 root root   534 May  4 03:56 A-2.txt\n",
            "-rw------- 1 root root 18459 May  4 03:56 F-1.txt\n",
            "-rw------- 1 root root  7264 May  4 03:56 F-2.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpZDGVBn-0lR",
        "colab_type": "text"
      },
      "source": [
        "**Load the text files**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Epv5-NIt1wIS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load txt file, one example per line\n",
        "def dataset_input_fn():\n",
        "  fullpaper_filenames = [\"drive/My Drive/Tensorflow/Data/Training/F-1.txt\",\"drive/My Drive/Tensorflow/Data/Training/F-2.txt\"]\n",
        "  abstract_filenames = [\"drive/My Drive/Tensorflow/Data/Training/A-1.txt\",\"drive/My Drive/Tensorflow/Data/Training/A-2.txt\"]\n",
        "  dataset_fullpapers = tf.data.Dataset.from_tensor_slices(fullpaper_filenames)\n",
        "  dataset_abstracts = tf.data.Dataset.from_tensor_slices(abstract_filenames)\n",
        "\n",
        "  dataset_fullpapers = dataset_fullpapers.map(\n",
        "    lambda filename: (\n",
        "        tf.io.read_file(filename)))\n",
        "  dataset_abstracts = dataset_abstracts.map(lambda filename: (\n",
        "        tf.io.read_file(filename)))\n",
        "  for data in dataset_abstracts:\n",
        "    print(data.shape)\n",
        "  # Zip the sentence and the labels together\n",
        "  training_dataset = tf.data.Dataset.zip((dataset_fullpapers, dataset_abstracts))\n",
        "  return training_dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDsCyXxZ-LXq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_dataset = dataset_input_fn() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "ec6a166c-b537-4012-8a97-b50310da4c52",
        "id": "Vh8Fo5mZJTxN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "source": [
        "!ls -la embeddings"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 841992\n",
            "drwxr-xr-x 3 root root      4096 May  3 14:53 .\n",
            "drwxr-xr-x 1 root root      4096 May  3 15:00 ..\n",
            "drwxr-xr-x 2 root root      4096 May  3 14:54 glove.6B\n",
            "-rw-r--r-- 1 root root 862182613 May  3 14:53 glove.6B.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlBDo4VpDaks",
        "colab_type": "code",
        "outputId": "1582803f-d705-4d2a-b6b9-f2baf4f31268",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0-alpha0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UD90Rd86GW1W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#datasetelement: each element in the dataset is a pair of string(fullpaper,abstract).\n",
        "#The number of elements in the dataset is the number of papers. In the testing, there are 2 papers => dataset has 2 elements.\n",
        "#then we call the function to transform:\n",
        "#The fullpaper (max 5000 words) => 5000 x 300 = Tensor (5000,300)\n",
        "#and abstract (max 250 words) => 250 x 300 = Tensor (250,300)\n",
        "#into matrix of embedding vectors (300 dimensions each vector), one vector per word.\n",
        "def prepare_embedding_dataset_fn(papertext,abstracttext):   \n",
        "  wordids_fullpaper,_ = text_to_word_ids(format(papertext),word_to_index) \n",
        "  fullpaper_embedding = ids_to_embeddings(wordids_fullpaper)  \n",
        "  wordids_abstract,_ = text_to_word_ids(format(abstracttext),word_to_index)  \n",
        "  abstract_embedding = ids_to_embeddings(wordids_abstract)\n",
        " \n",
        "  return fullpaper_embedding,abstract_embedding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3GEaqMMMxRA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for papertext,abstracttext in training_dataset:\n",
        "    fullpaper_embedding,abstract_embedding = prepare_embedding_dataset_fn(papertext,abstracttext) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRyoAz95vVfs",
        "colab_type": "text"
      },
      "source": [
        "**Create the models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8HaQG6L4fvB",
        "colab_type": "text"
      },
      "source": [
        "**Conditional GAN's**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mp9IdvT64oCu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = 400\n",
        "BATCH_SIZE = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0GoF8K6BD7F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def downsample(filters, size, apply_batchnorm=True):\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "  result = tf.keras.Sequential()\n",
        "  result.add(\n",
        "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
        "                             kernel_initializer=initializer, use_bias=False))\n",
        "\n",
        "  if apply_batchnorm:\n",
        "    result.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "  result.add(tf.keras.layers.LeakyReLU())\n",
        "\n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgnjNr3kKGUk",
        "colab_type": "text"
      },
      "source": [
        "**Testing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REbJ_n_-BHjl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "down_model = downsample(300, 4)\n",
        "for fullpaper,abstract in training_dataset:\n",
        "  word_ids_list, resulttext = text_to_word_ids(format(fullpaper),word_to_index)\n",
        "  abstract_word_ids_list, resulttextabstract = text_to_word_ids(format(abstract),word_to_index)\n",
        "  print(\"word_ids_list length ={}\".format(len(word_ids_list)))\n",
        "  # 4096 words x 300 dimension embedding vector => Tensor [64,64,300]\n",
        "  test_fullpaper_embedding = ids_to_embeddings_V2(word_ids_list,4096)\n",
        "  test_abstract_embedding = ids_to_embeddings_V2(abstract_word_ids_list,256)\n",
        "  print(\"test_fullpaper_embedding length={}\".format(len(test_fullpaper_embedding)))\n",
        " \n",
        "  test = tf.convert_to_tensor(test_fullpaper_embedding)\n",
        "  print(\"test_fullpaper_embedding convert to tensor with dtype={}\".format(test.dtype))\n",
        "  test = tf.cast(test, tf.float32)\n",
        "  print(\"test_fullpaper_embedding convert to tensor with with cast, dtype={}\".format(test.dtype))\n",
        "  testvector = tf.reshape(test,[64,64,300])\n",
        "  print(\"testvector type = {}\".format(testvector.dtype))\n",
        "  print(\"testvector shape = {}\".format(testvector.shape))\n",
        "  test_input = tf.expand_dims(testvector, 0)\n",
        "  print(\"test_input type = {}\".format(test_input.dtype))\n",
        "  print(\"test_input shape = {}\".format(test_input.shape))\n",
        "  down_result = down_model(test_input)\n",
        "  print (down_result.shape)\n",
        "    \n",
        "  test_abstract_tensor = tf.convert_to_tensor(test_abstract_embedding)\n",
        "  test_abstract_tensor = tf.cast(test_abstract_tensor, tf.float32)\n",
        "  test_abstract_vector = tf.reshape(test_abstract_tensor,[16,16,300])\n",
        "  test_abstract_vector_input = tf.expand_dims(test_abstract_vector, 0)\n",
        " \n",
        "  down_abstract_model = downsample(300, 3)\n",
        "  down_abstract_result = down_abstract_model(test_abstract_vector_input)\n",
        "  print (down_abstract_result.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSRTFpinYmW0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def padWordIndex(word_ids_array,padToSize):\n",
        "  resultarray = []\n",
        "  currentwordlen = len (word_ids_array)\n",
        "  if currentwordlen == padToSize: # no need to pad\n",
        "    resultarray = word_ids_array\n",
        "  elif currentwordlen > padToSize: # already greater, then trim down\n",
        "    for i in range(0,padToSize):\n",
        "      resultarray.append(word_ids_array[i])\n",
        "  else: # pad up to the padToSize\n",
        "    for i in range(0,currentwordlen):\n",
        "      resultarray.append(word_ids_array[i])\n",
        "    for j in range(currentwordlen,padToSize):\n",
        "      resultarray.append(400000) # UNKNOW WORD INDEX\n",
        "  return resultarray  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xywJjQZUWJXF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "down_text = downsample(64,2)\n",
        "for fullpaper,abstract in training_dataset:\n",
        "  word_ids_list, resulttext = text_to_word_ids(format(fullpaper),word_to_index)\n",
        "  testpadarray = padWordIndex(word_ids_list,4096)\n",
        "  print(len(testpadarray))\n",
        "  testpadarray = tf.cast(testpadarray,tf.float32)\n",
        "  word_ids_vector = tf.reshape(testpadarray,[64,64,1])\n",
        "  word_ids_vector = tf.expand_dims(word_ids_vector, 0)\n",
        "  down_text_result = down_text(word_ids_vector)\n",
        "  print(down_text_result.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HRfySyTmx9q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def upsample(filters, size, apply_dropout=False):\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "  result = tf.keras.Sequential()\n",
        "  result.add(\n",
        "    tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
        "                                    padding='same',\n",
        "                                    kernel_initializer=initializer,\n",
        "                                    use_bias=False))\n",
        "  \n",
        "  result.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "  if apply_dropout:\n",
        "      result.add(tf.keras.layers.Dropout(0.5))\n",
        "\n",
        "  result.add(tf.keras.layers.ReLU())\n",
        "\n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRGLgpWam1kY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "24a1a0d9-25f0-4fe9-80e3-ecf0ccebb537"
      },
      "source": [
        "up_model = upsample(300, 4)\n",
        "up_result = up_model(down_result)\n",
        "print (up_result.shape)\n"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 64, 64, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNLs2mGbpOX7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#number of embedding vector from Glove\n",
        "EMBEDDING_DIMS = 300"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHzrWFODootC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Generator():\n",
        "  down_stack = [\n",
        "    downsample(300, 4, apply_batchnorm=False), # (bs, 32, 32, 300)\n",
        "    downsample(300, 4), # (bs, 16, 16, 300)\n",
        "    downsample(300, 4), # (bs, 8, 8, 300)\n",
        "    downsample(300, 4), # (bs, 4, 4, 300)\n",
        "    downsample(300, 4), # (bs, 2, 2, 300)\n",
        "    downsample(300, 4) # (bs, 1, 1, 300)\n",
        "    #downsample(512, 4), # (bs, 2, 2, 512)\n",
        "    #downsample(512, 4), # (bs, 1, 1, 512)\n",
        "  ]\n",
        "\n",
        "  up_stack = [\n",
        "    upsample(300, 4, apply_dropout=True), # (bs, 2, 2, 300)\n",
        "    upsample(300, 4, apply_dropout=True), # (bs, 4, 4, 300)\n",
        "    upsample(300, 4, apply_dropout=True),    # (bs, 8, 8, 300)\n",
        "    #upsample(300, 4), # (bs, 16, 16, 300)\n",
        "    #upsample(300, 4), # (bs, 32, 32, 300)\n",
        "    #upsample(300, 4) # (bs, 64, 64, 300)\n",
        "    #upsample(64, 4), # (bs, 128, 128, 128)\n",
        "  ]\n",
        "\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "  last = tf.keras.layers.Conv2DTranspose(EMBEDDING_DIMS, 4,\n",
        "                                         strides=(2,2),\n",
        "                                         padding='same',\n",
        "                                         dilation_rate=(1, 1),\n",
        "                                         kernel_initializer=initializer,\n",
        "                                         activation='tanh') # (bs, 16, 16, 300)\n",
        "\n",
        "  concat = tf.keras.layers.Concatenate()\n",
        "\n",
        "  inputs = tf.keras.layers.Input(shape=[None,None,300])\n",
        "  x = inputs\n",
        "\n",
        "  # Downsampling through the model\n",
        "  skips = []\n",
        "  for down in down_stack:\n",
        "    x = down(x)\n",
        "    skips.append(x)\n",
        "\n",
        "  skips = reversed(skips[:-1])\n",
        "\n",
        "  # Upsampling and establishing the skip connections\n",
        "  for up, skip in zip(up_stack, skips):\n",
        "    x = up(x)\n",
        "    x = concat([x, skip])\n",
        "\n",
        "  x = last(x)\n",
        "\n",
        "  return tf.keras.Model(inputs=inputs, outputs=x)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APZ3PFiCuxAs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator = Generator()\n",
        "gen_output = generator(test_input, training=False)\n",
        "print(\"gen_output shape ={}\".format(gen_output.shape))\n",
        "#print(gen_output[0])\n",
        "#plt.imshow(gen_output[0,...])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yok8UGQ9UiQr",
        "colab_type": "text"
      },
      "source": [
        "**Build the Discriminator**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkeToGg6UkmA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Discriminator():\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "  inp = tf.keras.layers.Input(shape=[None, None, 300], name='input_abstract')\n",
        "  tar = tf.keras.layers.Input(shape=[None, None, 300], name='target_abstract')\n",
        "\n",
        "  x = tf.keras.layers.concatenate([inp, tar]) # (bs, 16, 16, 300)\n",
        "\n",
        "  #down1 = downsample(300, 4, False)(x) # (bs, 8, 8, 300)\n",
        "  #down2 = downsample(300, 4)(down1) # (bs, 4, 4, 300)\n",
        "  #down3 = downsample(300, 4)(down2) # (bs, 2, 2, 300)\n",
        "\n",
        "  #zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3) # (bs, 4,4, 300)\n",
        "  zero_pad1 = x\n",
        "  conv = tf.keras.layers.Conv2D(300, 4, strides=1,\n",
        "                                kernel_initializer=initializer,\n",
        "                                use_bias=False)(zero_pad1) # (bs, 31, 31, 300)\n",
        "\n",
        "  batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n",
        "\n",
        "  leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n",
        "\n",
        "  zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu) # (bs, 33, 33, 300)\n",
        "\n",
        "  last = tf.keras.layers.Conv2D(1, 4, strides=1,\n",
        "                                kernel_initializer=initializer)(zero_pad2) # (bs, 12, 12, 1)\n",
        "\n",
        "  return tf.keras.Model(inputs=[inp, tar], outputs=last)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TT-kLFs1snr_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "discriminator = Discriminator()\n",
        "disc_out = discriminator([test_abstract_vector, gen_output], training=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZSBoUIXt8-b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "33dfc329-5160-4864-8289-c88b0ef7a108"
      },
      "source": [
        "print(\"disc_out:\")\n",
        "print(disc_out.shape)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "disc_out:\n",
            "(1, 12, 12, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzC9KPkJvNVY",
        "colab_type": "text"
      },
      "source": [
        "**Loss function and optimizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CD6fXaaHvSbe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LAMBDA = 100\n",
        "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "def discriminator_loss(disc_real_output, disc_generated_output):\n",
        "  real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
        "\n",
        "  generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
        "\n",
        "  total_disc_loss = real_loss + generated_loss\n",
        "\n",
        "  return total_disc_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i42dmISIvZFj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generator_loss(disc_generated_output, gen_output, target):\n",
        "  gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
        "\n",
        "  # mean absolute error\n",
        "  l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
        "\n",
        "  total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n",
        "\n",
        "  return total_gen_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kJUb37UvcES",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdmAdzaqvfCw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVtzIVpavjSq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 2 #200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeUORPaRv35H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(input_fullpaper, target_abstract):\n",
        "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "    gen_output = generator(input_fullpaper, training=True)\n",
        "\n",
        "    disc_real_output = discriminator([target_abstract, target_abstract], training=True)\n",
        "    disc_generated_output = discriminator([target_abstract, gen_output], training=True)\n",
        "\n",
        "    gen_loss = generator_loss(disc_generated_output, gen_output, target_abstract)\n",
        "    disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
        "\n",
        "  generator_gradients = gen_tape.gradient(gen_loss,\n",
        "                                          generator.trainable_variables)\n",
        "  discriminator_gradients = disc_tape.gradient(disc_loss,\n",
        "                                               discriminator.trainable_variables)\n",
        "\n",
        "  generator_optimizer.apply_gradients(zip(generator_gradients,\n",
        "                                          generator.trainable_variables))\n",
        "  discriminator_optimizer.apply_gradients(zip(discriminator_gradients,\n",
        "                                              discriminator.trainable_variables))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsCDCUCRv5VU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(dataset, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "\n",
        "    for input_fullpaper, target_abstract in dataset:\n",
        "      embedding_fullpaper,embedding_abstract = get_embedding_pair(input_fullpaper,target_abstract)\n",
        "      train_step(embedding_fullpaper, embedding_abstract)\n",
        "\n",
        "    #clear_output(wait=True)\n",
        "    #for inp, tar in test_dataset.take(1):\n",
        "    #  generate_images(generator, inp, tar)\n",
        "\n",
        "    # saving (checkpoint) the model every 20 epochs\n",
        "    if (epoch + 1) % 20 == 0:\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n",
        "                                                        time.time()-start))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcQ1cz_hx_Bw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_embedding_pair(input_fullpaper,target_abstract):\n",
        "  fullpaper_word_ids_list, resulttext = text_to_word_ids(format(input_fullpaper),word_to_index)\n",
        "  abstract_word_ids_list, resulttextabstract = text_to_word_ids(format(target_abstract),word_to_index)\n",
        "   \n",
        "  fullpaper_embedding = ids_to_embeddings_V2(fullpaper_word_ids_list,4096)\n",
        "  abstract_embedding = ids_to_embeddings_V2(abstract_word_ids_list,256)  \n",
        "  \n",
        "  fullpaper_embedding_tensor = tf.convert_to_tensor(test_fullpaper_embedding)   \n",
        "  fullpaper_embedding_tensor = tf.cast(fullpaper_embedding_tensor, tf.float32)   \n",
        "  fullpaper_output = tf.reshape(fullpaper_embedding_tensor,[64,64,300]) \n",
        "  fullpaper_output = tf.expand_dims(fullpaper_output, 0)\n",
        "   \n",
        "  \n",
        "  abstract_tensor = tf.convert_to_tensor(abstract_embedding)\n",
        "  abstract_tensor = tf.cast(abstract_tensor, tf.float32)\n",
        "  abstract_tensor = tf.reshape(abstract_tensor,[16,16,300])\n",
        "  abstract_output = tf.expand_dims(abstract_tensor, 0)\n",
        "  \n",
        "  return fullpaper_output,abstract_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RH-zrf1wLdd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "a4862fc0-a2c5-499b-a209-9aba077b90d7"
      },
      "source": [
        "train(training_dataset, EPOCHS)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time taken for epoch 1 is 0.4783625602722168 sec\n",
            "\n",
            "Time taken for epoch 2 is 0.43973422050476074 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}